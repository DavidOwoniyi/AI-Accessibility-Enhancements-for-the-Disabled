{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Accessibility Enhancements for the Disabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr  # For voice command recognition\n",
    "import time  # To handle timing for gesture detection\n",
    "from gtts import gTTS  # Google Text-to-Speech for converting text to speech\n",
    "from playsound import playsound  # To play the generated speech\n",
    "import os  # For handling directory and file operations\n",
    "import pickle  # For saving data and labels to a file\n",
    "import mediapipe as mp  # For hand landmark detection\n",
    "import cv2  # OpenCV for image processing\n",
    "from sklearn.ensemble import RandomForestClassifier  # For building the classification model\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.metrics import accuracy_score  # For evaluating the model's accuracy\n",
    "import pickle  # For loading and saving serialized objects\n",
    "import numpy as np  # For numerical operations and array handling\n",
    "import pyautogui  # For automating GUI interactions like taking screenshots\n",
    "import webbrowser  # For opening web pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign language recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for class h\n",
      "Starting image capture for class h\n",
      "Collecting data for class e\n",
      "Starting image capture for class e\n",
      "Collecting data for class l\n",
      "Starting image capture for class l\n",
      "Collecting data for class d\n",
      "Starting image capture for class d\n",
      "Collecting data for class o\n",
      "Starting image capture for class o\n",
      "Collecting data for class w\n",
      "Starting image capture for class w\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where the dataset will be stored\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# Check if the data directory exists, if not, create it\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "# List of class labels, representing the different gestures or characters to be captured\n",
    "class_labels = [\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', \n",
    "    'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', \n",
    "    '6', '7', '8', '9', '-', '!'\n",
    "]  # Example class labels\n",
    "\n",
    "# Set the number of images to capture per class\n",
    "dataset_size = 300\n",
    "\n",
    "# Open a connection to the default camera (webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop through each class label to capture images for each one\n",
    "for class_index in range(len(class_labels)):\n",
    "    class_name = class_labels[class_index]\n",
    "    \n",
    "    # Create a directory for each class if it doesn't already exist\n",
    "    class_dir = os.path.join(DATA_DIR, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "\n",
    "    # Notify the user which class is being captured\n",
    "    print('Collecting data for class {}'.format(class_name))\n",
    "\n",
    "    # Wait for the user to press 'r' to start capturing images for the current class\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Capture a frame from the camera\n",
    "\n",
    "        # Check if the frame was successfully captured\n",
    "        if not ret:\n",
    "            break  # Exit the loop if the frame was not captured\n",
    "\n",
    "        # Display a message on the video feed to prompt the user to start capturing\n",
    "        cv2.putText(\n",
    "            frame, \"Ready? Press 'r' to start capturing!\", \n",
    "            (25, 60), cv2.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 255), 2, cv2.LINE_AA\n",
    "        )\n",
    "        cv2.imshow(\"Capturing datasets\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(25)\n",
    "\n",
    "        # Start capturing images when 'r' is pressed\n",
    "        if key == ord('r'):\n",
    "            print('Starting image capture for class {}'.format(class_name))\n",
    "            break  # Exit the loop and start capturing images\n",
    "        elif key == ord('x'):\n",
    "            # Release the camera and close all OpenCV windows if 'x' is pressed\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break  # Exit the entire program if 'x' is pressed\n",
    "\n",
    "    # Image capture process: capture images until the desired dataset size is reached\n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()  # Capture a frame from the camera\n",
    "\n",
    "        # Check if the frame was successfully captured\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame during capture\")\n",
    "            break  # Exit the loop if the frame was not captured\n",
    "\n",
    "        # Display the current frame\n",
    "        cv2.imshow(\"Capturing datasets\", frame)\n",
    "        cv2.waitKey(25)\n",
    "\n",
    "        # Save the captured frame to the class directory\n",
    "        cv2.imwrite(os.path.join(class_dir, '{}.jpg'.format(counter)), frame)\n",
    "        counter += 1\n",
    "\n",
    "# Release the camera and close all OpenCV windows when done\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Set up the Hands object for static image mode with a minimum detection confidence\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Define the directory where the image data is stored\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# Initialize lists to store data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each sub-directory in the data directory\n",
    "for sub_dir in os.listdir(DATA_DIR):\n",
    "    sub_dir_path = os.path.join(DATA_DIR, sub_dir)\n",
    "    if not os.path.isdir(sub_dir_path):\n",
    "        continue  # Skip if it's not a directory\n",
    "    \n",
    "    # Loop through each image in the sub-directory\n",
    "    for img_path in os.listdir(sub_dir_path):\n",
    "        img_path_full = os.path.join(sub_dir_path, img_path)\n",
    "        # print(f\"Loading image from: {img_path_full}\")\n",
    "\n",
    "        # Read the image using OpenCV\n",
    "        img = cv2.imread(img_path_full)\n",
    "        if img is None:\n",
    "            # print(f\"Failed to load image at: {img_path_full}\")\n",
    "            continue  # Skip if the image cannot be loaded\n",
    "\n",
    "        # Convert the image to RGB format for processing with MediaPipe\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)  # Process the image to detect hand landmarks\n",
    "\n",
    "        # Check if any hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                x_ = []\n",
    "                y_ = []\n",
    "                data_aux = []\n",
    "\n",
    "                # Collect x and y coordinates of the hand landmarks\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                # Normalize the coordinates by subtracting the minimum values\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "                # Append the normalized data to the data list\n",
    "                data.append(data_aux)\n",
    "                # Append the corresponding label (sub-directory name) to the labels list\n",
    "                labels.append(sub_dir)\n",
    "\n",
    "# Save the data and labels to a pickle file\n",
    "f = open('data.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 42\n",
      "100.0% of smaples were classified correctly !\n"
     ]
    }
   ],
   "source": [
    "# Load the data from a pickle file\n",
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "\n",
    "# Determine the maximum sequence length in the data\n",
    "max_len = max(len(x) for x in data_dict['data'])\n",
    "print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "# Pad the sequences in the data to ensure they all have the same length\n",
    "padded_data = np.array([np.pad(x, (0, max_len - len(x)), 'constant') for x in data_dict['data']])\n",
    "\n",
    "# Convert labels to a NumPy array for consistency\n",
    "labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "# Initialize a Random Forest Classifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "# Calculate the accuracy of the model's predictions\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print('{}% of samples were classified correctly!'.format(score * 100))\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "f = open('model.p', 'wb')\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions & detect hand gestures with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model from a file\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']  # Extract the model from the loaded dictionary\n",
    "\n",
    "# Initialize the webcam for video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands for hand detection and landmarks\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Define a dictionary mapping numeric labels to characters\n",
    "labels_dict = {\n",
    "    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G',\n",
    "    7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M',\n",
    "    13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S',\n",
    "    19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z',\n",
    "    26: '1', 27: '2', 28: '3', 29: '4', 30: '5', 31: '6',\n",
    "    32: '7', 33: '8', 34: '9', 35: '0', 36: '-', 37: '!'\n",
    "}\n",
    "\n",
    "# Start a loop to continuously capture frames from the webcam\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Capture a frame\n",
    "    if not ret:  # If the frame is not captured correctly, exit the loop\n",
    "        break\n",
    "\n",
    "    # Initialize lists to store auxiliary data and coordinates\n",
    "    data_aux = []\n",
    "    x_, y_ = [], []\n",
    "\n",
    "    # Convert the frame to RGB format for MediaPipe processing\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)  # Process the frame for hand landmarks\n",
    "\n",
    "    if results.multi_hand_landmarks:  # If hand landmarks are detected\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Extract and normalize landmark coordinates\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            # Prepare the feature vector by subtracting minimum coordinates\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "            # Make sure the length of features matches what the model expects\n",
    "            if len(data_aux) == 42:  # Adjust this according to your model's expected input length\n",
    "                prediction = model.predict([np.asarray(data_aux)])\n",
    "                predicted_character = prediction[0]  # Directly use the predicted string\n",
    "\n",
    "                # Draw bounding box and predicted character on the frame\n",
    "                x1, y1 = int(min(x_) * frame.shape[1]), int(min(y_) * frame.shape[0])\n",
    "                x2, y2 = int(max(x_) * frame.shape[1]), int(max(y_) * frame.shape[0])\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (25, 32, 48), 4)\n",
    "                cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_COMPLEX, 1.3, (25, 32, 48), 3, cv2.LINE_AA)\n",
    "\n",
    "            # Draw hand landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "            )\n",
    "\n",
    "    # Display the frame with the drawn landmarks and predictions\n",
    "    cv2.imshow('Sign Language Detector', frame)\n",
    "    key = cv2.waitKey(1)  # Wait for 1 ms for a key press\n",
    "\n",
    "    if key == ord('x'):  # Exit the loop if 'x' is pressed\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Hand Gestures into text & speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped frame due to incorrect number of features.\n",
      "Capturing started...\n",
      "Captured letter: m\n",
      "Captured letter: y\n",
      "Added space.\n",
      "Captured letter: n\n",
      "Captured letter: u\n",
      "Captured letter: m\n",
      "Captured letter: b\n",
      "Captured letter: e\n",
      "Captured letter: r\n",
      "Added space.\n",
      "Captured letter: i\n",
      "Captured letter: s\n",
      "Added space.\n",
      "Captured letter: 0\n",
      "Captured letter: 9\n",
      "Captured letter: 0\n",
      "Captured letter: 4\n",
      "Captured letter: 8\n",
      "Captured letter: 2\n",
      "Captured letter: 9\n",
      "Captured letter: 5\n",
      "Captured letter: 4\n",
      "Captured letter: 5\n",
      "Captured letter: 9\n",
      "Capturing stopped.\n",
      "Captured text: my number is 09048295459\n"
     ]
    }
   ],
   "source": [
    "# Function to convert text to speech\n",
    "def text_to_speech(text):\n",
    "    if not text.strip():  # Check if the text is empty or only whitespace\n",
    "        print(\"No text to convert to speech.\")\n",
    "        return\n",
    "    # Convert the text to speech using gTTS with Nigerian accent (tld='com.ng')\n",
    "    speech = gTTS(text, tld='us', lang='en', slow=False)\n",
    "    speech_file = 'speech.mp3'  # Temporary filename for the speech audio\n",
    "    speech.save(speech_file)  # Save the speech to the file\n",
    "    playsound(speech_file)  # Play the speech audio\n",
    "    os.remove(speech_file)  # Remove the audio file after playing\n",
    "\n",
    "# Function to get the predicted gesture from the frame\n",
    "def get_predicted_gesture(frame, hands, model, mp_drawing, mp_hands, mp_drawing_styles):\n",
    "    data_aux = []  # To store the normalized coordinates of landmarks\n",
    "    x_, y_ = [], []  # To store x and y coordinates separately\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert frame to RGB for processing\n",
    "    results = hands.process(frame_rgb)  # Process the frame to detect hand landmarks\n",
    "\n",
    "    if results.multi_hand_landmarks:  # If hand landmarks are detected\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Collect x and y coordinates of each landmark\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            # Normalize the coordinates and append to data_aux\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        # Check if the number of features matches the model's expectation\n",
    "        if len(data_aux) == model.n_features_in_:\n",
    "            prediction = model.predict([np.asarray(data_aux)])  # Predict the gesture\n",
    "            predicted_gesture = prediction[0]  # Get the predicted gesture\n",
    "\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw the hand landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "                )\n",
    "            return predicted_gesture  # Return the predicted gesture\n",
    "        else:\n",
    "            print(\"Skipped frame due to incorrect number of features.\")\n",
    "            return None\n",
    "\n",
    "    return None  # Return None if no hand landmarks are detected\n",
    "\n",
    "# Function to capture gestures, convert them to text, and then to speech\n",
    "def gesture_to_text_and_speech():\n",
    "    captured_text = \"\"  # String to store the captured text\n",
    "    capturing = False  # Boolean to track if capturing is ongoing\n",
    "    capture_start_time = None  # To store the start time of capturing\n",
    "\n",
    "    model_dict = pickle.load(open('./model.p', 'rb'))  # Load the trained model\n",
    "    model = model_dict['model']  # Extract the model from the dictionary\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Start video capture from the default camera\n",
    "\n",
    "    # Initialize MediaPipe components for hand detection and drawing\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.3)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()  # Capture a frame from the video feed\n",
    "            if not ret:  # Break if frame capture fails\n",
    "                break\n",
    "\n",
    "            predicted_gesture = get_predicted_gesture(frame, hands, model, mp_drawing, mp_hands, mp_drawing_styles)\n",
    "\n",
    "            if predicted_gesture:\n",
    "                if predicted_gesture == '!':  # Start or stop capturing based on the '!' gesture\n",
    "                    if not capturing:\n",
    "                        if capture_start_time is None:\n",
    "                            capture_start_time = time.time()\n",
    "                        elif time.time() - capture_start_time >= 2:\n",
    "                            capturing = True\n",
    "                            print(\"Capturing started...\")\n",
    "                            capture_start_time = None\n",
    "                    else:\n",
    "                        if capture_start_time is None:\n",
    "                            capture_start_time = time.time()\n",
    "                        elif time.time() - capture_start_time >= 2:\n",
    "                            capturing = False\n",
    "                            print(\"Capturing stopped.\")\n",
    "                            print(\"Captured text:\", captured_text)\n",
    "                            text_to_speech(captured_text)  # Convert captured text to speech\n",
    "                            captured_text = \"\"  # Reset captured text\n",
    "                            capture_start_time = None\n",
    "\n",
    "                elif capturing:\n",
    "                    if predicted_gesture == '-':  # Add a space to the captured text with '-' gesture\n",
    "                        if capture_start_time is None:\n",
    "                            capture_start_time = time.time()\n",
    "                        elif time.time() - capture_start_time >= 2:\n",
    "                            captured_text += \" \"\n",
    "                            print(\"Added space.\")\n",
    "                            capture_start_time = None\n",
    "                    else:\n",
    "                        if capture_start_time is None:\n",
    "                            capture_start_time = time.time()\n",
    "                        elif time.time() - capture_start_time >= 2:\n",
    "                            captured_text += predicted_gesture  # Append the captured letter\n",
    "                            print(\"Captured letter:\", predicted_gesture)\n",
    "                            capture_start_time = None\n",
    "\n",
    "            # Display the captured text on the frame\n",
    "            cv2.putText(frame, captured_text, (25, 60), cv2.FONT_HERSHEY_COMPLEX, 0.9, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"Gesture-Based Text and Speech\", frame)  # Show the frame with the captured text\n",
    "\n",
    "            key = cv2.waitKey(1)  # Check for key press\n",
    "            if key == ord('x'):  # Break the loop if 'x' is pressed\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()  # Release the video capture object\n",
    "        cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "\n",
    "# Run the gesture to text and speech conversion\n",
    "gesture_to_text_and_speech()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice command recognition & Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture voice input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the recognizer for speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def capture_voice_input():\n",
    "    \"\"\"\n",
    "    Captures voice input from the microphone and returns the audio data.\n",
    "    Handles ambient noise by adjusting the recognizer sensitivity.\n",
    "    \"\"\"\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)  # Adjust for background noise\n",
    "\n",
    "        try:\n",
    "            # Listen for the user's input, with a timeout for silence and maximum duration\n",
    "            audio = recognizer.listen(source, timeout=20, phrase_time_limit=12)\n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"Listening timed out while waiting for you to speak\")\n",
    "            return None  # Return None if the timeout is reached\n",
    "    return audio  # Return the captured audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text):\n",
    "    \"\"\"\n",
    "    Converts text to speech using Google Text-to-Speech (gTTS) and plays it.\n",
    "    The speech is saved to a temporary file and deleted after playback.\n",
    "    \"\"\"\n",
    "    speech = gTTS(text, tld='us', lang='en', slow=False)  # Create speech object\n",
    "    speech_file = 'speech.mp3'  # Define the filename for the audio file\n",
    "    speech.save(speech_file)  # Save the speech to the file\n",
    "    playsound('speech.mp3')  # Play the audio file\n",
    "    os.remove(speech_file)  # Remove the file after playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Voice to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_voice_to_text(audio):\n",
    "    \"\"\"\n",
    "    Converts the captured audio to text using Google's speech recognition.\n",
    "    Handles errors like unrecognized speech and request errors.\n",
    "    \"\"\"\n",
    "    if audio is None:\n",
    "        return \"\"  # Return an empty string if no audio is captured\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)  # Convert audio to text\n",
    "        print(\"You said: \" + text)\n",
    "    except sr.UnknownValueError:\n",
    "        text = \"\"\n",
    "        print(\"Sorry I didn't understand that.\")\n",
    "        text_to_speech(f\"Sorry I didn't understand that\")\n",
    "    except sr.RequestError as e:\n",
    "        text = \"\"\n",
    "        print(\"Error: {0}\".format(e))  # Print the error if the API request fails\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Voice Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_voice_command(text):\n",
    "    \"\"\"\n",
    "    Processes the recognized text and performs specific actions based on voice commands.\n",
    "    \"\"\"\n",
    "    if \"hello\" in text.lower():\n",
    "        print(\"Hello! How can I help you?\")\n",
    "        text_to_speech(f\"Hello! How can I help you\")\n",
    "\n",
    "    elif \"what is your name\" in text.lower():\n",
    "        print(\"My name is Jarvis\")\n",
    "        text_to_speech(f\"My name is Jarvis\")\n",
    "\n",
    "    elif (\"how are you doing today\" in text.lower() or\n",
    "          \"how are you\" in text.lower() or\n",
    "          \"how are you doing\" in text.lower()):\n",
    "        print(\"I'm doing alright, thank you very much\")\n",
    "        text_to_speech(f\"I'm doing alright, thank you very much\")\n",
    "\n",
    "    elif (\"take a screenshot\" in text.lower() or \n",
    "          \"take screenshot\" in text.lower() or\n",
    "          \"screenshot\" in text.lower()):\n",
    "        pyautogui.screenshot(\"screenshot.png\")  # Take a screenshot and save it\n",
    "        print(\"I took a screenshot for you\")\n",
    "        text_to_speech(f\"I took a screenshot for you\")\n",
    "\n",
    "    elif \"open youtube\" in text.lower():\n",
    "        print(\"Opening YouTube\")\n",
    "        webbrowser.open(\"https://www.youtube.com/\")  # Open YouTube in the web browser\n",
    "        text_to_speech(f\"Opening YouTube\")\n",
    "\n",
    "    elif (\"read the news\" in text.lower() or \n",
    "          \"the news\" in text.lower() or\n",
    "          \"news\" in text.lower() or\n",
    "          \"open news\" in text.lower()):\n",
    "        print(\"Opening the news\")\n",
    "        webbrowser.open(\"https://punchng.com/\")  # Open a news website\n",
    "        text_to_speech(f\"Opening the news\")\n",
    "\n",
    "    elif (\"alright, goodbye\" in text.lower() or \n",
    "          \"alright\" in text.lower() or \n",
    "          \"goodbye\" in text.lower() or \n",
    "          \"all right\" in text.lower() or \n",
    "          \"stop\" in text.lower()):\n",
    "        print(\"Goodbye! Have a nice day\")\n",
    "        text_to_speech(f\"Goodbye! Have a nice day\")\n",
    "        return True  # End the program\n",
    "    else: \n",
    "        print(\"I didn't understand that command. Please try again.\") \n",
    "        text_to_speech(f\"I didn't understand that command. Please try again.\")\n",
    "    return False  # Continue running the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "You said: hello how are you doing\n",
      "Hello! How can I help you?\n",
      "Listening...\n",
      "You said: open the news\n",
      "Opening the news\n",
      "Listening...\n",
      "You said: alright goodbye\n",
      "Goodbye! Have a nice day\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main loop for capturing voice input, processing commands, and handling retries.\n",
    "    The loop exits after a successful command or after a set number of failed attempts.\n",
    "    \"\"\"\n",
    "    end_program = False  # Flag to indicate if the program should end\n",
    "    attempts = 0  # Track the number of failed attempts\n",
    "    max_attempts = 3  # Limit to the number of attempts\n",
    "\n",
    "    while not end_program and attempts < max_attempts:\n",
    "        audio = capture_voice_input()  # Capture voice input\n",
    "        if audio is None:\n",
    "            attempts += 1\n",
    "            print(f\"Retrying... ({attempts}/{max_attempts})\")\n",
    "            time.sleep(1)  # Delay to prevent rapid looping\n",
    "        else:\n",
    "            text = convert_voice_to_text(audio)  # Convert voice input to text\n",
    "            if text == \"\":\n",
    "                attempts += 1\n",
    "                print(f\"Retrying... ({attempts}/{max_attempts})\")\n",
    "            else:\n",
    "                end_program = process_voice_command(text)  # Process the voice command\n",
    "                attempts = 0  # Reset attempts if a valid command is processed\n",
    "    if attempts >= max_attempts:\n",
    "        print(\"Too many failed attempts due to timeout. Exiting program.\")\n",
    "        text_to_speech(f\"Too many failed attempts due to timeout.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Run the main function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
