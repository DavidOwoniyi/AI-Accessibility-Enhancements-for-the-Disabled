{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessibility Improvements for the Disabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import os\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign language recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for class a\n",
      "Starting image capture for class a\n",
      "Collecting data for class b\n",
      "Starting image capture for class b\n",
      "Collecting data for class c\n",
      "Starting image capture for class c\n",
      "Collecting data for class d\n",
      "Starting image capture for class d\n",
      "Collecting data for class e\n",
      "Starting image capture for class e\n",
      "Collecting data for class f\n",
      "Starting image capture for class f\n",
      "Collecting data for class g\n",
      "Starting image capture for class g\n",
      "Collecting data for class h\n",
      "Starting image capture for class h\n",
      "Collecting data for class i\n",
      "Starting image capture for class i\n",
      "Collecting data for class j\n",
      "Starting image capture for class j\n",
      "Collecting data for class k\n",
      "Starting image capture for class k\n",
      "Collecting data for class l\n",
      "Starting image capture for class l\n",
      "Collecting data for class m\n",
      "Starting image capture for class m\n",
      "Collecting data for class n\n",
      "Starting image capture for class n\n",
      "Collecting data for class o\n",
      "Starting image capture for class o\n",
      "Collecting data for class p\n",
      "Starting image capture for class p\n",
      "Collecting data for class q\n",
      "Starting image capture for class q\n",
      "Collecting data for class r\n",
      "Starting image capture for class r\n",
      "Collecting data for class s\n",
      "Starting image capture for class s\n",
      "Collecting data for class t\n",
      "Starting image capture for class t\n",
      "Collecting data for class u\n",
      "Starting image capture for class u\n",
      "Collecting data for class v\n",
      "Starting image capture for class v\n",
      "Collecting data for class w\n",
      "Starting image capture for class w\n",
      "Collecting data for class x\n",
      "Starting image capture for class x\n",
      "Collecting data for class y\n",
      "Starting image capture for class y\n",
      "Collecting data for class z\n",
      "Starting image capture for class z\n",
      "Collecting data for class 1\n",
      "Starting image capture for class 1\n",
      "Collecting data for class 2\n",
      "Starting image capture for class 2\n",
      "Collecting data for class 3\n",
      "Starting image capture for class 3\n",
      "Collecting data for class 4\n",
      "Starting image capture for class 4\n",
      "Collecting data for class 5\n",
      "Starting image capture for class 5\n",
      "Collecting data for class 6\n",
      "Starting image capture for class 6\n",
      "Collecting data for class 7\n",
      "Starting image capture for class 7\n",
      "Collecting data for class 8\n",
      "Starting image capture for class 8\n",
      "Collecting data for class 9\n",
      "Starting image capture for class 9\n",
      "Collecting data for class 10\n",
      "Starting image capture for class 10\n",
      "Collecting data for class -\n",
      "Starting image capture for class -\n",
      "Collecting data for class !\n",
      "Starting image capture for class !\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "class_labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n",
    "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', '!']  # Example class labels\n",
    "\n",
    "dataset_size = 300\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "for class_index in range(len(class_labels)):\n",
    "    class_name = class_labels[class_index]\n",
    "    \n",
    "    class_dir = os.path.join(DATA_DIR, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "\n",
    "    print('Collecting data for class {}'.format(class_name))\n",
    "\n",
    "    # Wait for the 'r' key press to start capturing\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Check if frame was successfully captured\n",
    "        if not ret:\n",
    "            break  # Exit the loop if the frame was not captured\n",
    "\n",
    "        cv2.putText(frame, \"Ready? Press 'r' to start capturing!\", (25, 60), cv2.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Capturing datasets\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(25)\n",
    "\n",
    "\n",
    "        if key == ord('r'):\n",
    "            print('Starting image capture for class {}'.format(class_name))\n",
    "            break  # Exit the loop and start capturing images\n",
    "        elif key == ord('x'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break  # Exit the entire program if 'x' is pressed\n",
    "\n",
    "    # Image capture process\n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Check if frame was successfully captured\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame during capture\")\n",
    "            break  # Exit the loop if the frame was not captured\n",
    "\n",
    "        cv2.imshow(\"Capturing datasets\", frame)\n",
    "        cv2.waitKey(25)\n",
    "        cv2.imwrite(os.path.join(class_dir, '{}.jpg'.format(counter)), frame)\n",
    "        counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for sub_dir in os.listdir(DATA_DIR):\n",
    "    sub_dir_path = os.path.join(DATA_DIR, sub_dir)\n",
    "    if not os.path.isdir(sub_dir_path):\n",
    "        continue\n",
    "    for img_path in os.listdir(sub_dir_path):\n",
    "        img_path_full = os.path.join(sub_dir_path, img_path)\n",
    "        # print(f\"Loading image from: {img_path_full}\")\n",
    "\n",
    "        img = cv2.imread(img_path_full)\n",
    "        if img is None:\n",
    "            # print(f\"Failed to load image at: {img_path_full}\")\n",
    "            continue\n",
    "\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                x_ = []\n",
    "                y_ = []\n",
    "                data_aux = []\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "                data.append(data_aux)\n",
    "                labels.append(sub_dir)\n",
    "\n",
    "f = open('data.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 42\n",
      "99.78012313104662% of smaples were classified correctly !\n"
     ]
    }
   ],
   "source": [
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "\n",
    "# data = np.asarray(data_dict['data'])\n",
    "# labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "# Determine the maximum length\n",
    "max_len = max(len(x) for x in data_dict['data'])\n",
    "print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "# Pad the sequences to ensure consistent shape\n",
    "padded_data = np.array([np.pad(x, (0, max_len - len(x)), 'constant') for x in data_dict['data']])\n",
    "\n",
    "# Convert labels to a NumPy array as well\n",
    "labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "print('{}% of smaples were classified correctly !'.format(score * 100))\n",
    "\n",
    "f = open('model.p', 'wb')\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "labels_dict = {\n",
    "    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G',\n",
    "    7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M',\n",
    "    13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S',\n",
    "    19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z',\n",
    "    26: '1', 27: '2', 28: '3', 29: '4', 30: '5', 31: '6',\n",
    "    32: '7', 33: '8', 34: '9', 35: '0', 36: '-', 37: '!'\n",
    "}\n",
    "\n",
    "while True:\n",
    "\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  # image to draw\n",
    "                hand_landmarks,  # model output\n",
    "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "                # mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "            \n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "        predicted_character = prediction[0]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (25, 32, 48), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_COMPLEX, 1.3, (25, 32, 48), 3, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        \n",
    "    cv2.imshow('Sign Language Detector', frame)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    if key == ord('x'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break  # Exit the entire program if 'x' is pressed\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice recognition and Command Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture voice input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def capture_voice_input():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)\n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"Listening timed out while waiting for you to speak\")\n",
    "            return None\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text):\n",
    "    # Initialize gTTS with the text to convert\n",
    "    speech = gTTS(text, tld='com.ng', lang='en', slow=False)\n",
    "\n",
    "    # Save the audio file to a temporary file\n",
    "    speech_file = 'speech.mp3'\n",
    "    speech.save(speech_file)\n",
    "\n",
    "    # Play the sound\n",
    "    playsound('speech.mp3')\n",
    "\n",
    "    # Remove the file after playing\n",
    "    os.remove(speech_file)\n",
    "    \n",
    "# text_to_speech(\"Oh my God, what are you doing, stop that!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Voice to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_voice_to_text(audio):\n",
    "    if audio is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"You said: \" + text)\n",
    "        text_to_speech(f\"You said {text}\")\n",
    "    except sr.UnknownValueError:\n",
    "        text = \"\"\n",
    "        print(\"Sorry I didn't understand that.\")\n",
    "        text_to_speech(f\"Sorry I didn't understand that\")\n",
    "    except sr.RequestError as e:\n",
    "        text = \"\"\n",
    "        print(\"Error: {0}\".format(e))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Voice Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_voice_command(text):\n",
    "    if \"hello\" in text.lower():\n",
    "        print(\"Hello! How can I help you?\")\n",
    "        text_to_speech(f\"Hello! How can I help you\")\n",
    "\n",
    "    elif \"what is your name\" in text.lower():\n",
    "        print(\"My name is Eden\")\n",
    "        text_to_speech(f\"My name is Eden\")\n",
    "\n",
    "    elif (\"how are you doing today\" in text.lower() or\n",
    "          \"how are you\" in text.lower() or\n",
    "          \"how are you doing\" in text.lower()):\n",
    "        print(\"I'm doing alright, thank you very much\")\n",
    "        text_to_speech(f\"I'm doing  alright, thank you very much\")\n",
    "\n",
    "    elif (\"alright, goodbye\" in text.lower() or \n",
    "          \"alright\" in text.lower() or \n",
    "          \"goodbye\" in text.lower() or \n",
    "          \"all right\" in text.lower() or \n",
    "          \"stop\" in text.lower()):\n",
    "        print(\"Goodbye! Have a nice day\")\n",
    "        text_to_speech(f\"Goodbye! Have a nice day\")\n",
    "        \n",
    "        return True\n",
    "    else: \n",
    "        print(\"I didn't understand that command. Please try again.\") \n",
    "        text_to_speech(f\"I didn't understand that command. Please try again.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Sorry I didn't understand that.\n",
      "Retrying... (1/3)\n",
      "Listening...\n",
      "You said: hello how are you doing today\n",
      "Hello! How can I help you?\n",
      "Listening...\n",
      "You said: it is enough good bad\n",
      "I didn't understand that command. Please try again.\n",
      "Listening...\n",
      "You said: all right goodbye you like this time\n",
      "Goodbye! Have a nice day\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    end_program = False\n",
    "    attempts = 0\n",
    "    max_attempts = 3  # Limit to the number of attempts\n",
    "\n",
    "    while not end_program and attempts < max_attempts:\n",
    "        audio = capture_voice_input()\n",
    "        if audio is None:\n",
    "            attempts += 1\n",
    "            print(f\"Retrying... ({attempts}/{max_attempts})\")\n",
    "            time.sleep(1)  # Delay to prevent rapid looping\n",
    "        else:\n",
    "            text = convert_voice_to_text(audio)\n",
    "            if text == \"\":\n",
    "                attempts += 1\n",
    "                print(f\"Retrying... ({attempts}/{max_attempts})\")\n",
    "            else:\n",
    "                end_program = process_voice_command(text)\n",
    "\n",
    "                attempts = 0  # Reset attempts if a valid audio input is processed\n",
    "    if attempts >= max_attempts:\n",
    "        print(\"Too many failed attempts due to timeout. Exiting program.\")\n",
    "        text_to_speech(f\"Too many failed attempts due to timeout. Exiting program.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
